# file transfer between linux servers in the same local area network

 use scp

    scp -r -P {port_number} {user_name}@{ip_address}:{source file path} {path in local to receive files}

for example, if we want to copy file in the server which is "amax@10.73.3.223 -P 52022" to our machine, and the files we want to copy is  under root "~/Desktop/zzh/VWW_Code/data_set/visual_wake_words/vww", we should code as below:
    
    scp -r -P 52022 amax@10.73.3.223:~/Desktop/zzh/VWW_Code/data_set/visual_wake_words/vww ./data_set/


# Distributed training

reference: [TensorFlow 分布式训练](https://tf.wiki/zh_hans/appendix/distributed.html)

what we want to do is use multi-gpus in the same machine.

so we use tf.distribute.MirroredStrategy 

examples:

    import tensorflow as tf
    import tensorflow_datasets as tfds
    
    num_epochs = 5 
    batch_size_per_replica = 64
    learning_rate = 0.001
	
    strategy = tf.distribute.MirroredStrategy()
    print('Number of devices: %d' % strategy.num_replicas_in_sync)  # 输出设备数量
    batch_size = batch_size_per_replica * strategy.num_replicas_in_sync
	
    # 载入数据集并预处理
    def resize(image, label):
        image = tf.image.resize(image, [224, 224]) / 255.0
        return image, label
	
    # 使用 TensorFlow Datasets 载入猫狗分类数据集，详见“TensorFlow Datasets数据集载入”一章
    dataset = tfds.load("cats_vs_dogs", split=tfds.Split.TRAIN, as_supervised=True)
    dataset = dataset.map(resize).shuffle(1024).batch(batch_size)
    
    with strategy.scope():
        model = tf.keras.applications.MobileNetV2(weights=None,      classes=2)
        model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),
        loss=tf.keras.losses.sparse_categorical_crossentropy,
        metrics=[tf.keras.metrics.sparse_categorical_accuracy]
    )

    model.fit(dataset, epochs=num_epochs)

we can choose specific gpu deviece

    strategy = tf.distribute.MirroredStrategy(devices=["/gpu:0", "/gpu:1"])